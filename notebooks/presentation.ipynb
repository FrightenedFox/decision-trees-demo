{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Random Forest and Extra Trees as examples of the ensemble methods\n",
    "\n",
    "## Introduction \n",
    "\n",
    "Both algorithms are so called ensemble methods. These are techniques that combine the predictions from multiple machine learning models to produce a single, more accurate result. The idea is that a group of models (ensemble) working together will outperform any individual model.\n",
    "\n",
    "Both Random Forests and Extra Trees are ensemble methods that use multiple **decision trees** as their base models. In other words, decision trees are the building blocks of these algorithms.\n",
    "\n",
    "How do they work?\n",
    "- Instead of building one decision tree, these algorithms build a **forest** of trees and aggregate the results.\n",
    "- Making Predictions:\n",
    "  - **Classification Tasks:** Each tree in the ensemble predicts a class label. The final prediction is made by **majority voting**â€”the class that gets the most votes from all the trees is chosen.\n",
    "  - **Regression Tasks:** Each tree predicts a numerical value. The final prediction is the **average** of all the tree predictions.\n",
    "- Finally, the predictions from all the trees are **aggregated** to make a final prediction. This process helps to reduce overfitting and improves generalization to unseen data.\n",
    "\n",
    "For better understanding of the algorithms, I would recommend watching the following videos from Normalized Nerd YT channel:\n",
    "- [Decision Trees](https://youtu.be/ZVR2Way4nwQ)\n",
    "- [Classification Random Forest](https://youtu.be/v6VJ2RO66Ag)\n",
    "- [Regression Random Forest](https://youtu.be/UhY5vPfQIrA)\n",
    "\n",
    "Here are some key screenshots from the videos:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Decision Tree Classification](../images/decision-tree-classification.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Decision Tree Split Selection](../images/decision-tree-classification-split-selection.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Decision Tree Split Selection](../images/decision-tree-regression-split-selection.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Decision Tree Regression](../images/decision-tree-regression-aggregation.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Random Forest Classification](../images/random-forest-classification.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here are some key differences between Random Forest and Extra Trees:\n",
    "\n",
    "## Random Forests:\n",
    "\n",
    "**Data Sampling:** Random Forests use a technique called *bootstrap aggregation* or *bagging*. This means that each tree in the forest is trained on a random subset of the original data, created by sampling with replacement. So, some data points might appear multiple times in a subset, and some might not appear at all.\n",
    "\n",
    "**Feature Selection and Splitting:**\n",
    "- At each node (decision point) in a tree, a random subset of features is selected.\n",
    "- The algorithm then looks for the best possible split among these features by evaluating all possible thresholds (e.g., for \"age,\" it might consider \"age > 30,\" \"age > 35,\" etc.).\n",
    "- The split that best separates the data based on a criterion (like Gini impurity or information gain) is chosen.\n",
    "\n",
    "## Extremely Randomized Trees (Extra Trees):\n",
    "\n",
    "**Data Sampling:** Extra Trees use the *entire* original dataset to train each tree, without any bootstrapping. So, every tree sees all the data points.\n",
    "\n",
    "**Feature Selection and Splitting:**\n",
    "- At each node, a random subset of features is still selected, just like in Random Forests.\n",
    "- **However, the key difference is in how splits are decided:**\n",
    "  - For each of these randomly chosen features, a split point (threshold) is selected **randomly**, not based on the best possible split.\n",
    "  - Among these randomly generated splits, the one that provides the best separation (according to the same criteria used in Random Forests) is chosen.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "import kagglehub\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from collections import Counter\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier, ExtraTreesClassifier\n",
    "from sklearn.metrics import classification_report, confusion_matrix, ConfusionMatrixDisplay, roc_curve, auc\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "from sklearn.preprocessing import label_binarize\n",
    "from sklearn import tree\n",
    "from imblearn.over_sampling import SMOTE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hands-on example using the Wine Quality dataset\n",
    "\n",
    "In this notebook, I will show you how to use Random Forest and Extra Trees for classification tasks using the `scikit-learn` library. We will use the [Wine Quality dataset](https://www.kaggle.com/datasets/yasserh/wine-quality-dataset) for this purpose."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download the latest version of the dataset\n",
    "path = kagglehub.dataset_download(\"yasserh/wine-quality-dataset\")\n",
    "\n",
    "print(\"Path to dataset files:\", path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Information about the dataset from Kaggle:\n",
    "\n",
    "**Description:**\n",
    "\n",
    "This datasets is related to red variants of the Portuguese \"Vinho Verde\" wine.The dataset describes the amount of various chemicals present in wine and their effect on it's quality. The datasets can be viewed as classification or regression tasks. The classes are ordered and **not balanced (e.g. there are much more normal wines than excellent or poor ones)**.\n",
    "\n",
    "A simple yet challenging project, to anticipate the quality of wine.\n",
    "The complexity arises due to the fact that the dataset has fewer samples, & is highly imbalanced.\n",
    "\n",
    "**This data frame contains the following columns:**\n",
    "\n",
    "Input variables (based on physicochemical tests):\\\n",
    "1 - fixed acidity\\\n",
    "2 - volatile acidity\\\n",
    "3 - citric acid\\\n",
    "4 - residual sugar\\\n",
    "5 - chlorides\\\n",
    "6 - free sulfur dioxide\\\n",
    "7 - total sulfur dioxide\\\n",
    "8 - density\\\n",
    "9 - pH\\\n",
    "10 - sulphates\\\n",
    "11 - alcohol\n",
    "\n",
    "Output variable (based on sensory data):\\\n",
    "12 - quality (score between 0 and 10)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### EDA and Data Preprocessing\n",
    "\n",
    "Let's start by loading the data and performing some exploratory data analysis (EDA) and data preprocessing steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!ls -l $path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the dataset and display the first few rows\n",
    "wine_data_path = path + \"/WineQT.csv\"\n",
    "wine_df = pd.read_csv(wine_data_path)\n",
    "wine_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display basic information\n",
    "wine_df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wine_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the distribution of the 'quality' variable\n",
    "wine_df.quality.hist(bins=6)\n",
    "wine_df.quality.value_counts().sort_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separate features and target variable\n",
    "X = wine_df.drop(['quality', 'Id'], axis=1)\n",
    "y = wine_df['quality']\n",
    "\n",
    "# Stratified train-test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42, stratify=y\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training and test set sizes\n",
    "pd.concat([y_train.value_counts(), y_test.value_counts()], axis=1, keys=['Train', 'Test']).sort_index()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialize, fit and predict using Random Forest and Extra Trees\n",
    "\n",
    "Next, we will initialize, fit, and predict using Random Forest and Extra Trees classifiers. We will compare the performance of both algorithms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize, fit and predict using the classifiers\n",
    "rf_classifier = RandomForestClassifier(random_state=42)\n",
    "et_classifier = ExtraTreesClassifier(random_state=42)\n",
    "\n",
    "rf_classifier.fit(X_train, y_train)\n",
    "et_classifier.fit(X_train, y_train)\n",
    "\n",
    "y_pred_rf = rf_classifier.predict(X_test)\n",
    "y_pred_et = et_classifier.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Random Forest Classification Report:\")\n",
    "print(classification_report(y_test, y_pred_rf))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cm_rf = confusion_matrix(y_test, y_pred_rf, labels=rf_classifier.classes_)\n",
    "disp_rf = ConfusionMatrixDisplay(confusion_matrix=cm_rf, display_labels=rf_classifier.classes_)\n",
    "disp_rf.plot(cmap='Blues')\n",
    "plt.title(\"Random Forest Confusion Matrix\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Extra Trees Classification Report:\")\n",
    "print(classification_report(y_test, y_pred_et))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cm_et = confusion_matrix(y_test, y_pred_et, labels=et_classifier.classes_)\n",
    "disp_et = ConfusionMatrixDisplay(confusion_matrix=cm_et, display_labels=et_classifier.classes_)\n",
    "disp_et.plot(cmap='Blues')\n",
    "plt.title(\"Extra Trees Confusion Matrix\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "importances_rf = rf_classifier.feature_importances_\n",
    "indices_rf = np.argsort(importances_rf)[::-1]\n",
    "features = X.columns\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.title(\"Random Forest Feature Importances\")\n",
    "sns.barplot(x=importances_rf[indices_rf], y=features[indices_rf], hue=features[indices_rf], legend=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classes = sorted(y.unique())\n",
    "y_test_binarized = label_binarize(y_test, classes=classes)\n",
    "n_classes = y_test_binarized.shape[1]\n",
    "y_score_rf = rf_classifier.predict_proba(X_test)\n",
    "y_score_et = et_classifier.predict_proba(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize dictionaries to store ROC curves and AUC scores\n",
    "fpr_rf = dict()\n",
    "tpr_rf = dict()\n",
    "roc_auc_rf = dict()\n",
    "\n",
    "fpr_et = dict()\n",
    "tpr_et = dict()\n",
    "roc_auc_et = dict()\n",
    "\n",
    "for i in range(n_classes):\n",
    "    fpr_rf[i], tpr_rf[i], _ = roc_curve(y_test_binarized[:, i], y_score_rf[:, i])\n",
    "    roc_auc_rf[i] = auc(fpr_rf[i], tpr_rf[i])\n",
    "    \n",
    "    fpr_et[i], tpr_et[i], _ = roc_curve(y_test_binarized[:, i], y_score_et[:, i])\n",
    "    roc_auc_et[i] = auc(fpr_et[i], tpr_et[i])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(21, 9))\n",
    "\n",
    "for i in range(n_classes):\n",
    "    ax1.plot(fpr_rf[i], tpr_rf[i], lw=2,\n",
    "             label='Class {0} (area = {1:0.2f})'.format(classes[i], roc_auc_rf[i]))\n",
    "    ax2.plot(fpr_et[i], tpr_et[i], lw=2,\n",
    "             label='Class {0} (area = {1:0.2f})'.format(classes[i], roc_auc_et[i]))\n",
    "\n",
    "ax1.set_title('Random Forest ROC Curves')\n",
    "ax2.set_title('Extra Trees ROC Curves')\n",
    "\n",
    "ax1.plot([0, 1], [0, 1], 'k--', lw=2)\n",
    "ax2.plot([0, 1], [0, 1], 'k--', lw=2)\n",
    "ax1.legend(loc='lower right')\n",
    "ax2.legend(loc='lower right')\n",
    "\n",
    "plt.xlim([-0.01, 1.01])\n",
    "plt.ylim([-0.01, 1.01])\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract a single tree\n",
    "estimator_rf = rf_classifier.estimators_[0]\n",
    "\n",
    "plt.figure(figsize=(20, 10))\n",
    "tree.plot_tree(estimator_rf,\n",
    "               feature_names=features,\n",
    "               class_names=[str(c) for c in classes],\n",
    "               filled=True,\n",
    "               rounded=True)\n",
    "plt.title(\"Decision Tree from Random Forest\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Try some balancing techniques to improve the model performance\n",
    "\n",
    "Since the dataset is imbalanced and the results of the raw models aren't impressive, we will try some balancing techniques to improve the model performance. First, we will use the `SMOTE` technique to oversample the minority classes. Then, we will try changing the class weights in the models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Counter(y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Desired number of samples per class\n",
    "sampling_strategy = {\n",
    "    3: 30,  \n",
    "    4: 60,  \n",
    "    5: Counter(y_train)[5],  \n",
    "    6: Counter(y_train)[6],  \n",
    "    7: 200, \n",
    "    8: 30\n",
    "}\n",
    "\n",
    "# Initialize and apply SMOTE to training data\n",
    "smote = SMOTE(sampling_strategy=sampling_strategy, random_state=42, k_neighbors=4)\n",
    "X_resampled, y_resampled = smote.fit_resample(X_train, y_train)\n",
    "\n",
    "# Resampled training set class distribution\n",
    "Counter(y_resampled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize, fit and predict using new classifiers\n",
    "rf_classifier_smote = RandomForestClassifier(random_state=42)\n",
    "et_classifier_smote = ExtraTreesClassifier(random_state=42)\n",
    "\n",
    "rf_classifier_smote.fit(X_resampled, y_resampled)\n",
    "et_classifier_smote.fit(X_resampled, y_resampled)\n",
    "\n",
    "y_pred_rf_smote = rf_classifier_smote.predict(X_test)\n",
    "y_pred_et_smote = et_classifier_smote.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Random Forest Classification Report (After SMOTE):\")\n",
    "print(classification_report(y_test, y_pred_rf_smote))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cm_rf_smote = confusion_matrix(y_test, y_pred_rf_smote, labels=rf_classifier_smote.classes_)\n",
    "disp_rf_smote = ConfusionMatrixDisplay(confusion_matrix=cm_rf_smote, display_labels=rf_classifier_smote.classes_)\n",
    "disp_rf_smote.plot(cmap='Blues')\n",
    "plt.title(\"Random Forest Confusion Matrix (After SMOTE)\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Extra Trees Classification Report (After SMOTE):\")\n",
    "print(classification_report(y_test, y_pred_et_smote))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cm_rf_smote = confusion_matrix(y_test, y_pred_rf_smote, labels=rf_classifier_smote.classes_)\n",
    "disp_rf_smote = ConfusionMatrixDisplay(confusion_matrix=cm_rf_smote, display_labels=rf_classifier_smote.classes_)\n",
    "disp_rf_smote.plot(cmap='Blues')\n",
    "plt.title(\"Random Forest Confusion Matrix (After SMOTE)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute class weights\n",
    "classes = np.unique(y_train)\n",
    "class_weights = compute_class_weight(\n",
    "    class_weight='balanced',\n",
    "    classes=classes,\n",
    "    y=y_train\n",
    ")\n",
    "class_weight_dict = dict(zip(classes, class_weights))\n",
    "\n",
    "class_weight_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize, fit and predict using classifiers with class weights\n",
    "rf_classifier_weighted = RandomForestClassifier(\n",
    "    class_weight=class_weight_dict,\n",
    "    random_state=42\n",
    ")\n",
    "et_classifier_weighted = ExtraTreesClassifier(\n",
    "    class_weight=class_weight_dict,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "rf_classifier_weighted.fit(X_train, y_train)\n",
    "et_classifier_weighted.fit(X_train, y_train)\n",
    "\n",
    "y_pred_rf_weighted = rf_classifier_weighted.predict(X_test)\n",
    "y_pred_et_weighted = et_classifier_weighted.predict(X_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Random Forest Classification Report (With Class Weights):\")\n",
    "print(classification_report(y_test, y_pred_rf_weighted))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cm_rf_weighted = confusion_matrix(y_test, y_pred_rf_weighted, labels=rf_classifier_weighted.classes_)\n",
    "disp_rf_weighted = ConfusionMatrixDisplay(confusion_matrix=cm_rf_weighted, display_labels=rf_classifier_weighted.classes_)\n",
    "disp_rf_weighted.plot(cmap='Blues')\n",
    "plt.title(\"Random Forest Confusion Matrix (With Class Weights)\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Extra Trees Classification Report (With Class Weights):\")\n",
    "print(classification_report(y_test, y_pred_et_weighted))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cm_et_weighted = confusion_matrix(y_test, y_pred_et_weighted, labels=et_classifier_weighted.classes_)\n",
    "disp_et_weighted = ConfusionMatrixDisplay(confusion_matrix=cm_et_weighted, display_labels=et_classifier_weighted.classes_)\n",
    "disp_et_weighted.plot(cmap='Blues')\n",
    "plt.title(\"Extra Trees Confusion Matrix (With Class Weights)\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
